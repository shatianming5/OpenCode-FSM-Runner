version: 1

# Example pipeline for deploy + rollout + benchmark automation.
#
# Runner executes commands in order:
# auth -> tests -> deploy.setup -> deploy.health -> rollout -> evaluation -> benchmark -> metrics validation.

security:
  mode: safe
  # allowlist: []   # optional: if set, ONLY matching regex commands are allowed
  # denylist: []    # optional: always blocked (in addition to safe defaults)
  max_cmd_seconds: 1800
  max_total_seconds: 7200

tests:
  # Keep this fast; treat it as a guardrail before deploy/benchmark.
  cmds:
    - pytest -q
  timeout_seconds: 1200

auth:
  # Optional: login steps if your deploy/benchmark pulls private images/models.
  # Use `--unattended guided` if any step is interactive.
  interactive: false
  # steps:
  #   - docker login --username ... --password-stdin

deploy:
  # Replace these with your benchmark deploy steps (k8s, compose, etc).
  # setup_cmds:
  #   - kubectl apply -f deploy/k8s/
  # health_cmds:
  #   - kubectl rollout status deploy/myapp --timeout=180s
  teardown_policy: on_failure
  # teardown_cmds:
  #   - kubectl delete -f deploy/k8s/ --ignore-not-found

  kubectl_dump:
    enabled: true
    # namespace: default
    # label_selector: app=myapp
    include_logs: true

rollout:
  # Optional: generate rollouts/trajectories.
  # run_cmds:
  #   - python -m your_project.rollout --out .aider_fsm/rollout.json
  # timeout_seconds: 3600
  # retries: 0

evaluation:
  # Replace with your evaluation entrypoint. It MUST write `metrics_path`.
  # run_cmds:
  #   - python -m your_project.eval.run --out .aider_fsm/metrics.json
  metrics_path: .aider_fsm/metrics.json
  required_keys:
    - score
    # - success_rate
    # - avg_reward

benchmark:
  # Optional: extra benchmark stage (if you set metrics_path here, it will also be validated).
  # run_cmds:
  #   - python -m your_project.bench.run --out .aider_fsm/bench_metrics.json
  # metrics_path: .aider_fsm/bench_metrics.json
  # required_keys: [score]

artifacts:
  out_dir: .aider_fsm/artifacts
